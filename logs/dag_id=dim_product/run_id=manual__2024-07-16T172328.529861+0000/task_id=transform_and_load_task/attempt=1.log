[2024-07-16T17:23:36.649+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dim_product.transform_and_load_task manual__2024-07-16T17:23:28.529861+00:00 [queued]>
[2024-07-16T17:23:36.665+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dim_product.transform_and_load_task manual__2024-07-16T17:23:28.529861+00:00 [queued]>
[2024-07-16T17:23:36.666+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2024-07-16T17:23:36.687+0000] {taskinstance.py:2191} INFO - Executing <Task(_PythonDecoratedOperator): transform_and_load_task> on 2024-07-16 17:23:28.529861+00:00
[2024-07-16T17:23:36.697+0000] {standard_task_runner.py:60} INFO - Started process 178 to run task
[2024-07-16T17:23:36.703+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'dim_product', 'transform_and_load_task', 'manual__2024-07-16T17:23:28.529861+00:00', '--job-id', '118', '--raw', '--subdir', 'DAGS_FOLDER/dim_product_orchestration.py', '--cfg-path', '/tmp/tmp5g1hh2n8']
[2024-07-16T17:23:36.705+0000] {standard_task_runner.py:88} INFO - Job 118: Subtask transform_and_load_task
[2024-07-16T17:23:36.817+0000] {task_command.py:423} INFO - Running <TaskInstance: dim_product.transform_and_load_task manual__2024-07-16T17:23:28.529861+00:00 [running]> on host 0d20fcb030df
[2024-07-16T17:23:37.027+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='dim_product' AIRFLOW_CTX_TASK_ID='transform_and_load_task' AIRFLOW_CTX_EXECUTION_DATE='2024-07-16T17:23:28.529861+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-07-16T17:23:28.529861+00:00'
[2024-07-16T17:23:37.761+0000] {pipeline_options.py:923} WARNING - Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.
[2024-07-16T17:23:38.049+0000] {pipeline_options.py:923} WARNING - Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.
[2024-07-16T17:23:40.033+0000] {pipeline_options.py:923} WARNING - Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.
[2024-07-16T17:23:40.296+0000] {pipeline_options.py:923} WARNING - Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.
[2024-07-16T17:23:40.583+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dim_product_orchestration.py", line 64, in transform_and_load_task
    (
  File "/home/airflow/.local/lib/python3.8/site-packages/apache_beam/pipeline.py", line 613, in __exit__
    self.result = self.run()
  File "/home/airflow/.local/lib/python3.8/site-packages/apache_beam/pipeline.py", line 560, in run
    return Pipeline.from_runner_api(
  File "/home/airflow/.local/lib/python3.8/site-packages/apache_beam/pipeline.py", line 587, in run
    return self.runner.run_pipeline(self, self._options)
  File "/home/airflow/.local/lib/python3.8/site-packages/apache_beam/runners/dataflow/dataflow_runner.py", line 395, in run_pipeline
    artifacts = environments.python_sdk_dependencies(options)
  File "/home/airflow/.local/lib/python3.8/site-packages/apache_beam/transforms/environments.py", line 905, in python_sdk_dependencies
    return stager.Stager.create_job_resources(
  File "/home/airflow/.local/lib/python3.8/site-packages/apache_beam/runners/portability/stager.py", line 279, in create_job_resources
    raise RuntimeError(
RuntimeError: The file /opt/***/plugins/my_modules/setup.py cannot be found. It was specified in the --setup_file command line option.
[2024-07-16T17:23:40.604+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=dim_product, task_id=transform_and_load_task, execution_date=20240716T172328, start_date=20240716T172336, end_date=20240716T172340
[2024-07-16T17:23:40.621+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 118 for task transform_and_load_task (The file /opt/***/plugins/my_modules/setup.py cannot be found. It was specified in the --setup_file command line option.; 178)
[2024-07-16T17:23:40.649+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-07-16T17:23:40.669+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
