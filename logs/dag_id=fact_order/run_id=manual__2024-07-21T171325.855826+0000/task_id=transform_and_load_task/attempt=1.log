[2024-07-21T17:13:40.801+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: fact_order.transform_and_load_task manual__2024-07-21T17:13:25.855826+00:00 [queued]>
[2024-07-21T17:13:40.813+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: fact_order.transform_and_load_task manual__2024-07-21T17:13:25.855826+00:00 [queued]>
[2024-07-21T17:13:40.814+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2024-07-21T17:13:40.832+0000] {taskinstance.py:2191} INFO - Executing <Task(_PythonDecoratedOperator): transform_and_load_task> on 2024-07-21 17:13:25.855826+00:00
[2024-07-21T17:13:40.841+0000] {standard_task_runner.py:60} INFO - Started process 8224 to run task
[2024-07-21T17:13:40.845+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'fact_order', 'transform_and_load_task', 'manual__2024-07-21T17:13:25.855826+00:00', '--job-id', '200', '--raw', '--subdir', 'DAGS_FOLDER/fact_order.py', '--cfg-path', '/tmp/tmps_70oqhq']
[2024-07-21T17:13:40.847+0000] {standard_task_runner.py:88} INFO - Job 200: Subtask transform_and_load_task
[2024-07-21T17:13:40.909+0000] {task_command.py:423} INFO - Running <TaskInstance: fact_order.transform_and_load_task manual__2024-07-21T17:13:25.855826+00:00 [running]> on host 087c584822ad
[2024-07-21T17:13:41.006+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='fact_order' AIRFLOW_CTX_TASK_ID='transform_and_load_task' AIRFLOW_CTX_EXECUTION_DATE='2024-07-21T17:13:25.855826+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-07-21T17:13:25.855826+00:00'
[2024-07-21T17:13:41.927+0000] {pipeline_options.py:923} WARNING - Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.
[2024-07-21T17:13:42.352+0000] {pipeline_options.py:923} WARNING - Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.
[2024-07-21T17:13:44.687+0000] {pipeline_options.py:923} WARNING - Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.
[2024-07-21T17:13:45.012+0000] {pipeline_options.py:923} WARNING - Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.
[2024-07-21T17:13:45.324+0000] {stager.py:800} INFO - Executing command: ['/usr/local/bin/python', '-m', 'build', '--no-isolation', '--sdist', '--outdir', '/tmp/tmpvazk5sri', '/opt/***/plugins']
[2024-07-21T17:13:45.392+0000] {stager.py:810} INFO - Executing command: ['/usr/local/bin/python', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmpvazk5sri']
[2024-07-21T17:13:48.131+0000] {dataflow_runner.py:397} INFO - Pipeline has additional dependencies to be installed in SDK worker container, consider using the SDK container image pre-building workflow to avoid repetitive installations. Learn more on https://cloud.google.com/dataflow/docs/guides/using-custom-containers#prebuild
[2024-07-21T17:13:48.135+0000] {environments.py:314} INFO - Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/beam_python3.8_sdk:2.57.0
[2024-07-21T17:13:48.136+0000] {environments.py:321} INFO - Python SDK container image set to "gcr.io/cloud-dataflow/v1beta3/beam_python3.8_sdk:2.57.0" for Docker environment
[2024-07-21T17:13:48.538+0000] {apiclient.py:663} INFO - Starting GCS upload to gs://ingestion_layer/staging/fact-order-bigquery-etl.1721582028.528183/workflow.tar.gz...
[2024-07-21T17:13:49.601+0000] {apiclient.py:673} INFO - Completed GCS upload to gs://ingestion_layer/staging/fact-order-bigquery-etl.1721582028.528183/workflow.tar.gz in 1 seconds.
[2024-07-21T17:13:49.602+0000] {apiclient.py:663} INFO - Starting GCS upload to gs://ingestion_layer/staging/fact-order-bigquery-etl.1721582028.528183/submission_environment_dependencies.txt...
[2024-07-21T17:13:50.490+0000] {apiclient.py:673} INFO - Completed GCS upload to gs://ingestion_layer/staging/fact-order-bigquery-etl.1721582028.528183/submission_environment_dependencies.txt in 0 seconds.
[2024-07-21T17:13:50.493+0000] {apiclient.py:663} INFO - Starting GCS upload to gs://ingestion_layer/staging/fact-order-bigquery-etl.1721582028.528183/pipeline.pb...
[2024-07-21T17:13:51.519+0000] {apiclient.py:673} INFO - Completed GCS upload to gs://ingestion_layer/staging/fact-order-bigquery-etl.1721582028.528183/pipeline.pb in 1 seconds.
[2024-07-21T17:13:51.544+0000] {pipeline_options.py:339} WARNING - Unknown pipeline options received: celery,worker. Ignore if flags are used for internal purposes.
[2024-07-21T17:13:51.553+0000] {pipeline_options.py:339} WARNING - Unknown pipeline options received: celery,worker. Ignore if flags are used for internal purposes.
[2024-07-21T17:13:53.015+0000] {apiclient.py:844} INFO - Create job: <Job
 clientRequestId: '20240721171348529440-8273'
 createTime: '2024-07-21T17:13:53.502492Z'
 currentStateTime: '1970-01-01T00:00:00Z'
 id: '2024-07-21_10_13_52-10498839913080032246'
 location: 'asia-southeast2'
 name: 'fact-order-bigquery-etl'
 projectId: 'liquid-kite-423215-s2'
 stageStates: []
 startTime: '2024-07-21T17:13:53.502492Z'
 steps: []
 tempFiles: []
 type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>
[2024-07-21T17:13:53.016+0000] {apiclient.py:846} INFO - Created job with id: [2024-07-21_10_13_52-10498839913080032246]
[2024-07-21T17:13:53.017+0000] {apiclient.py:847} INFO - Submitted job: 2024-07-21_10_13_52-10498839913080032246
[2024-07-21T17:13:53.018+0000] {apiclient.py:848} INFO - To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/asia-southeast2/2024-07-21_10_13_52-10498839913080032246?project=liquid-kite-423215-s2
[2024-07-21T17:13:53.386+0000] {dataflow_runner.py:153} INFO - Job 2024-07-21_10_13_52-10498839913080032246 is in state JOB_STATE_PENDING
[2024-07-21T17:13:58.555+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:13:56.928Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in asia-southeast2-c.
[2024-07-21T17:13:58.702+0000] {dataflow_runner.py:153} INFO - Job 2024-07-21_10_13_52-10498839913080032246 is in state JOB_STATE_RUNNING
[2024-07-21T17:14:03.876+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:13:59.097Z: JOB_MESSAGE_BASIC: Executing operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Impulse+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/FlatMap(<lambda at core.py:3788>)+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Map(decode)
[2024-07-21T17:14:03.889+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:13:59.109Z: JOB_MESSAGE_BASIC: Executing operation Start/Impulse+Start/FlatMap(<lambda at core.py:3788>)+Start/Map(decode)
[2024-07-21T17:14:03.890+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:13:59.120Z: JOB_MESSAGE_BASIC: Executing operation ReadAvro/ReadAvro/Read/Impulse+ReadAvro/ReadAvro/Read/EmitSource+ref_AppliedPTransform_ReadAvro-ReadAvro-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_13/PairWithRestriction+ref_AppliedPTransform_ReadAvro-ReadAvro-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_13/SplitWithSizing
[2024-07-21T17:14:03.892+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:13:59.132Z: JOB_MESSAGE_BASIC: Executing operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Impulse+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/FlatMap(<lambda at core.py:3788>)+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Map(decode)+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/GenerateFilePrefix+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/SchemaModJobNamePrefix
[2024-07-21T17:14:03.893+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:13:59.132Z: JOB_MESSAGE_BASIC: Starting 1 workers in asia-southeast2...
[2024-07-21T17:17:32.362+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:31.267Z: JOB_MESSAGE_BASIC: All workers have finished the startup processes and began to receive work requests.
[2024-07-21T17:17:37.664+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:33.639Z: JOB_MESSAGE_BASIC: Finished operation ReadAvro/ReadAvro/Read/Impulse+ReadAvro/ReadAvro/Read/EmitSource+ref_AppliedPTransform_ReadAvro-ReadAvro-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_13/PairWithRestriction+ref_AppliedPTransform_ReadAvro-ReadAvro-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_13/SplitWithSizing
[2024-07-21T17:17:37.665+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:33.691Z: JOB_MESSAGE_BASIC: Executing operation RemoveDuplicates/GroupByKey/Create
[2024-07-21T17:17:37.666+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.205Z: JOB_MESSAGE_BASIC: Finished operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Impulse+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/FlatMap(<lambda at core.py:3788>)+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ImpulseEmptyPC/Map(decode)
[2024-07-21T17:17:37.666+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.240Z: JOB_MESSAGE_BASIC: Finished operation Start/Impulse+Start/FlatMap(<lambda at core.py:3788>)+Start/Map(decode)
[2024-07-21T17:17:37.667+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.378Z: JOB_MESSAGE_BASIC: Finished operation RemoveDuplicates/GroupByKey/Create
[2024-07-21T17:17:37.667+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.430Z: JOB_MESSAGE_BASIC: Executing operation ref_AppliedPTransform_ReadAvro-ReadAvro-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_13/ProcessElementAndRestrictionWithSizing+RemoveDuplicates/PairWithKey+RemoveDuplicates/GroupByKey/Write
[2024-07-21T17:17:37.668+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.471Z: JOB_MESSAGE_BASIC: Finished operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Impulse+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/FlatMap(<lambda at core.py:3788>)+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ImpulseSingleElementPC/Map(decode)+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/GenerateFilePrefix+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/SchemaModJobNamePrefix
[2024-07-21T17:17:37.669+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.543Z: JOB_MESSAGE_BASIC: Executing operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)/View-python_side_input0
[2024-07-21T17:17:37.670+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.554Z: JOB_MESSAGE_BASIC: Executing operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/View-python_side_input0
[2024-07-21T17:17:37.670+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.564Z: JOB_MESSAGE_BASIC: Executing operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/View-python_side_input0
[2024-07-21T17:17:37.671+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.576Z: JOB_MESSAGE_BASIC: Executing operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/View-python_side_input0
[2024-07-21T17:17:37.672+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.588Z: JOB_MESSAGE_BASIC: Executing operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/ParDo(TriggerLoadJobs)/View-python_side_input0
[2024-07-21T17:17:37.672+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.590Z: JOB_MESSAGE_BASIC: Finished operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)/View-python_side_input0
[2024-07-21T17:17:37.673+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.597Z: JOB_MESSAGE_BASIC: Finished operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/View-python_side_input0
[2024-07-21T17:17:37.673+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.599Z: JOB_MESSAGE_BASIC: Executing operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/View-python_side_input0
[2024-07-21T17:17:37.674+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.614Z: JOB_MESSAGE_BASIC: Finished operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/View-python_side_input0
[2024-07-21T17:17:37.674+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.626Z: JOB_MESSAGE_BASIC: Finished operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/View-python_side_input0
[2024-07-21T17:17:37.675+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.634Z: JOB_MESSAGE_BASIC: Finished operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/ParDo(TriggerLoadJobs)/View-python_side_input0
[2024-07-21T17:17:37.676+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:34.652Z: JOB_MESSAGE_BASIC: Finished operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/View-python_side_input0
[2024-07-21T17:17:37.676+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:36.565Z: JOB_MESSAGE_BASIC: Finished operation ref_AppliedPTransform_ReadAvro-ReadAvro-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_13/ProcessElementAndRestrictionWithSizing+RemoveDuplicates/PairWithKey+RemoveDuplicates/GroupByKey/Write
[2024-07-21T17:17:37.677+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:36.590Z: JOB_MESSAGE_BASIC: Executing operation RemoveDuplicates/GroupByKey/Close
[2024-07-21T17:17:37.677+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:36.727Z: JOB_MESSAGE_BASIC: Finished operation RemoveDuplicates/GroupByKey/Close
[2024-07-21T17:17:37.678+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:36.752Z: JOB_MESSAGE_BASIC: Executing operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/GroupShardedRows/Create
[2024-07-21T17:17:37.679+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:36.841Z: JOB_MESSAGE_BASIC: Finished operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/GroupShardedRows/Create
[2024-07-21T17:17:37.680+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:36.895Z: JOB_MESSAGE_BASIC: Executing operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create
[2024-07-21T17:17:37.680+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:36.977Z: JOB_MESSAGE_BASIC: Finished operation WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create
[2024-07-21T17:17:37.681+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:37.027Z: JOB_MESSAGE_BASIC: Executing operation RemoveDuplicates/GroupByKey/Read+RemoveDuplicates/RemoveDuplicates+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/AppendDestination+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/GroupShardedRows/Write
[2024-07-21T17:17:37.682+0000] {dataflow_runner.py:207} ERROR - 2024-07-21T17:17:37.373Z: JOB_MESSAGE_ERROR: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 311, in _execute
    response = task()
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 386, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 656, in do_instruction
    return getattr(self, request_type)(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 694, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 1119, in process_bundle
    input_op_by_transform_id[element.transform_id].process_encoded(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 237, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 569, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 571, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 262, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 265, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 952, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 953, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1437, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1547, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000 [while running 'RemoveDuplicates/RemoveDuplicates-ptransform-63']

[2024-07-21T17:17:37.682+0000] {dataflow_runner.py:207} ERROR - 2024-07-21T17:17:37.573Z: JOB_MESSAGE_ERROR: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 311, in _execute
    response = task()
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 386, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 656, in do_instruction
    return getattr(self, request_type)(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 694, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 1119, in process_bundle
    input_op_by_transform_id[element.transform_id].process_encoded(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 237, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 569, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 571, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 262, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 265, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 952, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 953, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1437, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1547, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000 [while running 'RemoveDuplicates/RemoveDuplicates-ptransform-63']

[2024-07-21T17:17:37.683+0000] {dataflow_runner.py:207} ERROR - 2024-07-21T17:17:37.766Z: JOB_MESSAGE_ERROR: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 311, in _execute
    response = task()
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 386, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 656, in do_instruction
    return getattr(self, request_type)(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 694, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 1119, in process_bundle
    input_op_by_transform_id[element.transform_id].process_encoded(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 237, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 569, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 571, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 262, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 265, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 952, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 953, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1437, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1547, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000 [while running 'RemoveDuplicates/RemoveDuplicates-ptransform-63']

[2024-07-21T17:17:37.684+0000] {dataflow_runner.py:207} ERROR - 2024-07-21T17:17:37.960Z: JOB_MESSAGE_ERROR: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 311, in _execute
    response = task()
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 386, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 656, in do_instruction
    return getattr(self, request_type)(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 694, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 1119, in process_bundle
    input_op_by_transform_id[element.transform_id].process_encoded(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 237, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 569, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 571, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 262, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 265, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 952, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 953, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1437, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1547, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000 [while running 'RemoveDuplicates/RemoveDuplicates-ptransform-63']

[2024-07-21T17:17:37.685+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:38.015Z: JOB_MESSAGE_BASIC: Finished operation RemoveDuplicates/GroupByKey/Read+RemoveDuplicates/RemoveDuplicates+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/AppendDestination+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/GroupShardedRows/Write
[2024-07-21T17:17:37.686+0000] {dataflow_runner.py:207} ERROR - 2024-07-21T17:17:38.051Z: JOB_MESSAGE_ERROR: Workflow failed. Causes: S14:RemoveDuplicates/GroupByKey/Read+RemoveDuplicates/RemoveDuplicates+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/AppendDestination+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteToBigQuery/WriteToBigQuery/BigQueryBatchFileLoads/GroupShardedRows/Write failed., The job failed because a work item has failed 4 times. Look in previous log entries for the cause of each one of the 4 failures. If the logs only contain generic timeout errors related to accessing external resources, such as MongoDB, verify that the worker service account has permission to access the resource's subnetwork. For more information, see https://cloud.google.com/dataflow/docs/guides/common-errors. The work item was attempted on these workers: 

      Root cause: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 311, in _execute
    response = task()
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 386, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 656, in do_instruction
    return getattr(self, request_type)(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 694, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 1119, in process_bundle
    input_op_by_transform_id[element.transform_id].process_encoded(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 237, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 569, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 571, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 262, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 265, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 952, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 953, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1437, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1547, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000 [while running 'RemoveDuplicates/RemoveDuplicates-ptransform-63']

      Worker ID: fact-order-bigquery-etl-07211013-n7oa-harness-qfb7,

      Root cause: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 311, in _execute
    response = task()
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 386, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 656, in do_instruction
    return getattr(self, request_type)(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 694, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 1119, in process_bundle
    input_op_by_transform_id[element.transform_id].process_encoded(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 237, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 569, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 571, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 262, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 265, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 952, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 953, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1437, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1547, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000 [while running 'RemoveDuplicates/RemoveDuplicates-ptransform-63']

      Worker ID: fact-order-bigquery-etl-07211013-n7oa-harness-qfb7,

      Root cause: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 311, in _execute
    response = task()
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 386, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 656, in do_instruction
    return getattr(self, request_type)(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 694, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 1119, in process_bundle
    input_op_by_transform_id[element.transform_id].process_encoded(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 237, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 569, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 571, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 262, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 265, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 952, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 953, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1437, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1547, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000 [while running 'RemoveDuplicates/RemoveDuplicates-ptransform-63']

      Worker ID: fact-order-bigquery-etl-07211013-n7oa-harness-qfb7,

      Root cause: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 311, in _execute
    response = task()
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 386, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 656, in do_instruction
    return getattr(self, request_type)(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 694, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 1119, in process_bundle
    input_op_by_transform_id[element.transform_id].process_encoded(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 237, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 569, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 571, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 262, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 265, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 952, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 953, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1437, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1547, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000 [while running 'RemoveDuplicates/RemoveDuplicates-ptransform-63']

      Worker ID: fact-order-bigquery-etl-07211013-n7oa-harness-qfb7
[2024-07-21T17:17:42.991+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:17:38.306Z: JOB_MESSAGE_BASIC: Stopping worker pool...
[2024-07-21T17:18:25.473+0000] {dataflow_runner.py:203} INFO - 2024-07-21T17:18:23.357Z: JOB_MESSAGE_BASIC: Worker pool stopped.
[2024-07-21T17:19:02.868+0000] {dataflow_runner.py:153} INFO - Job 2024-07-21_10_13_52-10498839913080032246 is in state JOB_STATE_FAILED
[2024-07-21T17:19:03.285+0000] {dataflow_runner.py:806} ERROR - Console URL: https://console.cloud.google.com/dataflow/jobs/<RegionId>/2024-07-21_10_13_52-10498839913080032246?project=<ProjectId>
[2024-07-21T17:19:03.288+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/fact_order.py", line 72, in transform_and_load_task
    (
  File "/home/airflow/.local/lib/python3.8/site-packages/apache_beam/pipeline.py", line 614, in __exit__
    self.result.wait_until_finish()
  File "/home/airflow/.local/lib/python3.8/site-packages/apache_beam/runners/dataflow/dataflow_runner.py", line 807, in wait_until_finish
    raise DataflowRuntimeException(
apache_beam.runners.dataflow.dataflow_runner.DataflowRuntimeException: Dataflow pipeline failed. State: FAILED, Error:
Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 311, in _execute
    response = task()
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 386, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 656, in do_instruction
    return getattr(self, request_type)(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 694, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 1119, in process_bundle
    input_op_by_transform_id[element.transform_id].process_encoded(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 237, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 569, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 571, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 262, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 265, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 952, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 953, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1437, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1547, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000 [while running 'RemoveDuplicates/RemoveDuplicates-ptransform-63']

[2024-07-21T17:19:03.359+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=fact_order, task_id=transform_and_load_task, execution_date=20240721T171325, start_date=20240721T171340, end_date=20240721T171903
[2024-07-21T17:19:03.396+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 200 for task transform_and_load_task (Dataflow pipeline failed. State: FAILED, Error:
Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 311, in _execute
    response = task()
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 386, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 656, in do_instruction
    return getattr(self, request_type)(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py", line 694, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 1119, in process_bundle
    input_op_by_transform_id[element.transform_id].process_encoded(
  File "/usr/local/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py", line 237, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 569, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 571, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 262, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 265, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 952, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 953, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1437, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1547, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1435, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 639, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in _remove_duplicates
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/opt/***/plugins/my_modules/transformation_load_stage.py", line 34, in <lambda>
    latest_record = max(grouped_elements, key = lambda elem: int(time.mktime(datetime.strptime(elem['last_updated_at'], "%Y-%m-%d %H:%M:%S").timetuple())))
  File "/usr/local/lib/python3.8/_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "/usr/local/lib/python3.8/_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: .000000 [while running 'RemoveDuplicates/RemoveDuplicates-ptransform-63']
; 8224)
[2024-07-21T17:19:03.432+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-07-21T17:19:03.455+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
